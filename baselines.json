[{"review_id": "HygLj-cG9B", "review_sentences": ["The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions.\n\n", "The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to \"directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems.\" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.\n\n", "As such the paper is not convincing.", "On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?", "The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.\n\n", "The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning."], "rebuttal_sentences": ["Thanks for your feedback.", "We discuss each comment in the following:\n\n", "- The experiments are not large scale\n\n", "We respectfully disagree with the reviewer's main comment that the experiments are not large scale.", "One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).", "Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).", "Sure, this is not the scale of 80 million tiny images; but one wouldn\u2019t ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.\n\n", "Representation learning, the topic of this conference, has many facets.", "Learning representations from \u201cbig data\u201d (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.", "Both are valuable in different circumstances.\n\n", "- No substantiate insight with respect to NP-hard problems\n\n", "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.\n", "To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives.", "These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard.", "Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees.", "Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML.", "Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem.", "So powerful non-convex solvers might be of a significant advantage over convex relaxations.", "Our paper simply shows ONE example for this.\n\n", "- It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly?\n\n", "It would not be possible to set the input dimension the same as the embedding dimension.\n", "Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error.", "The size of the embedding dimension can be too low to achieve this.", "One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation.", "However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding.\n\n", "- Methods, where items have no representation, are questionable\n\n", "Items having no representation is a caveat of the data available rather than that of the method.", "The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.\n\n", "- How to generalize to unseen items\n\n", "First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.\n", "We believe that in our case, generalization is realizable.", "One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.", "The network can be trained with extra batches of triplets which involves the new items.\n\n", "- The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.\n\n", "We don\u2019t really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this."], "review_segmentations": {"label": [{"label": "arg_structuring|summary", "start": 0, "excl_end": 1}, {"label": "arg_evaluative", "start": 1, "excl_end": 3}, {"label": "arg_request|explanation", "start": 3, "excl_end": 4}, {"label": "arg_evaluative", "start": 4, "excl_end": 6}], "alignment": [{"label": "reb_idxs_2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19", "start": 1, "excl_end": 2}, {"label": "reb_idxs_20|21|22|23|24|25", "start": 3, "excl_end": 4}, {"label": "reb_idxs_26|27|28|29|30|31|32|33", "start": 4, "excl_end": 5}, {"label": "reb_idxs_34|35", "start": 5, "excl_end": 6}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 1}, {"label": "tt_segment_1", "start": 1, "excl_end": 2}, {"label": "tt_segment_2", "start": 2, "excl_end": 6}], "entity_grid": [{"label": "paper", "start": 0, "excl_end": 5}, {"label": "representation", "start": 0, "excl_end": 4}, {"label": "items", "start": 0, "excl_end": 4}, {"label": "learning", "start": 1, "excl_end": 5}, {"label": "it", "start": 1, "excl_end": 3}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 1}, {"label": "rebuttal_structuring", "start": 1, "excl_end": 3}, {"label": "rebuttal_reject-criticism", "start": 3, "excl_end": 10}, {"label": "rebuttal_structuring", "start": 10, "excl_end": 11}, {"label": "rebuttal_reject-criticism", "start": 11, "excl_end": 13}, {"label": "rebuttal_summary", "start": 13, "excl_end": 19}, {"label": "rebuttal_reject-criticism", "start": 19, "excl_end": 20}, {"label": "rebuttal_structuring", "start": 20, "excl_end": 21}, {"label": "rebuttal_answer", "start": 21, "excl_end": 26}, {"label": "rebuttal_structuring", "start": 26, "excl_end": 27}, {"label": "rebuttal_reject-criticism", "start": 27, "excl_end": 29}, {"label": "rebuttal_structuring", "start": 29, "excl_end": 30}, {"label": "rebuttal_reject-criticism", "start": 30, "excl_end": 34}, {"label": "rebuttal_structuring", "start": 34, "excl_end": 35}, {"label": "rebuttal_followup", "start": 35, "excl_end": 36}], "alignment": [{"label": "rev_idxs_1", "start": 2, "excl_end": 20}, {"label": "rev_idxs_3", "start": 20, "excl_end": 26}, {"label": "rev_idxs_4", "start": 26, "excl_end": 34}, {"label": "rev_idxs_5", "start": 34, "excl_end": 36}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 7}, {"label": "tt_segment_1", "start": 7, "excl_end": 20}, {"label": "tt_segment_2", "start": 20, "excl_end": 26}, {"label": "tt_segment_3", "start": 26, "excl_end": 34}, {"label": "tt_segment_4", "start": 34, "excl_end": 36}], "entity_grid": [{"label": "we", "start": 1, "excl_end": 35}, {"label": "comment", "start": 1, "excl_end": 3}, {"label": "experiments", "start": 2, "excl_end": 22}, {"label": "scale", "start": 2, "excl_end": 6}, {"label": "reviewer", "start": 3, "excl_end": 35}, {"label": "one", "start": 4, "excl_end": 24}, {"label": "embedding", "start": 4, "excl_end": 25}, {"label": "methods", "start": 4, "excl_end": 26}, {"label": "points", "start": 4, "excl_end": 5}, {"label": "approach", "start": 5, "excl_end": 32}, {"label": "triplets", "start": 5, "excl_end": 33}, {"label": "images", "start": 6, "excl_end": 8}, {"label": "instances", "start": 6, "excl_end": 30}, {"label": "representation", "start": 7, "excl_end": 32}, {"label": "learning", "start": 7, "excl_end": 35}, {"label": "representations", "start": 8, "excl_end": 22}, {"label": "data", "start": 8, "excl_end": 27}, {"label": "np", "start": 10, "excl_end": 14}, {"label": "problems", "start": 10, "excl_end": 30}, {"label": "networks", "start": 11, "excl_end": 12}, {"label": "instance", "start": 11, "excl_end": 13}, {"label": "optimization", "start": 11, "excl_end": 17}, {"label": "problem", "start": 11, "excl_end": 17}, {"label": "idea", "start": 12, "excl_end": 35}, {"label": "objectives", "start": 12, "excl_end": 13}, {"label": "convex", "start": 15, "excl_end": 18}, {"label": "relaxations", "start": 15, "excl_end": 18}, {"label": "practice", "start": 16, "excl_end": 30}, {"label": "advantage", "start": 18, "excl_end": 30}, {"label": "paper", "start": 19, "excl_end": 34}, {"label": "example", "start": 19, "excl_end": 30}, {"label": "it", "start": 20, "excl_end": 32}, {"label": "log", "start": 20, "excl_end": 22}, {"label": "n", "start": 20, "excl_end": 22}, {"label": "items", "start": 20, "excl_end": 33}, {"label": "input", "start": 21, "excl_end": 24}, {"label": "dimension", "start": 21, "excl_end": 23}, {"label": "size", "start": 22, "excl_end": 23}, {"label": "network", "start": 24, "excl_end": 33}, {"label": "method", "start": 25, "excl_end": 27}, {"label": "field", "start": 28, "excl_end": 34}, {"label": "generalization", "start": 30, "excl_end": 31}, {"label": "matrix", "start": 34, "excl_end": 35}, {"label": "factorization", "start": 34, "excl_end": 35}]}}, {"review_id": "S1xGt0Rq3m", "review_sentences": ["The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup.", "The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.\n\n", "Pros:\n", "1. A new few shot learning with domain shift problem is studied in the paper.\n", "2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario.", "The experimental improvements on omniglot seem quite substantial.\n\n", "Cons:\n", "1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?", "It seems that both are using meta-learning with domain adaptation technique.", "What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?", "I feel the baseline in domain adaptation area is a bit limited.\n", "2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?\n", "3. It seems the domain shift in the paper is less dramatic.", "i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.\n", "4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.\n\n", "Minor:\n", "1. Where is L_da in Figure 2? In Figure 2, what\u2019s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?\n", "2. In the caption of figure 2, there should be a space after `\":\"."], "rebuttal_sentences": ["Overall:\n", "We thank you for your time and appreciating the strengths of our work.", "Based on your guidance and suggestions, we have tried to do additional experiments to improve the quality of our work.\n\n", "Concern 1: Comparison to Meta-RevGrad\n", "Meta-RevGrad tries to achieve feature invariance at the embedding level.", "Achieving such feature invariance for high-dimensional feature mapping can be a very weak constraint [1], causing limitation in performance.", "Recently, generative approaches, following image-to-image translation have been shown achieve better domain adaptation, as this constrains the feature embedding to generate the data in a new domain.", "Being a non-GAN based approach, concepts such idt (encouraging the styling network to behave as an identity when given a target domain instance as input) and revMap (constructing source instance back from generated target instance) are not applicable in this scenario, as no instances or images are being generated from a feature embedding.\n\n", "Concern 1, 2, 3: Experiments\n", "Thank you for these suggestions.", "Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.", "Specifically:\n\n", "\u201cDomain Adaptation Baselines\u201d,\n", "We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.\n\n", "\u201cSimple baseline \u2013 combining a subset of a new domain as training set\u201d\n", "We see the merit of this baseline, but there are several challenges in executing this.", "Designing it in a fair way is tricky.", "Using some labelled data in target domain maybe unfair, as we are not allowed to see meta-test data.", "Moreover, this is likely to not work, as the meta-train data would be too large, and would dominate, and we do not have a clear way to set the weights.\n\n", "\u201cDramatic Domain Shift, Omniglot to Fashion-MNIST\u201d\n", "This could be an interesting setting, but we don\u2019t think this will work very well, as the tasks are themselves completely different. We would not expect a character recognition model to transfer to a object recognition task, as the visual features are very different.\n\n\n", "Minor:\n", "Thanks for this; we have updated the draft to make the presentation clearer.", "Unlabelled data refers to only the domain of the meta-test data, but the meta-test data is never used in meta-training.\n", "L_da is essentially the sum of L_gan and L_cycle.\n\n\n", "[1] Shu, R., Bui, H.H., Narui, H. and Ermon, S. A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018"], "review_segmentations": {"label": [{"label": "arg_structuring|summary", "start": 0, "excl_end": 2}, {"label": "arg_structuring|heading", "start": 2, "excl_end": 3}, {"label": "arg_evaluative", "start": 3, "excl_end": 6}, {"label": "arg_structuring|heading", "start": 6, "excl_end": 7}, {"label": "arg_request|clarification", "start": 7, "excl_end": 8}, {"label": "arg_fact", "start": 8, "excl_end": 9}, {"label": "arg_request|experiment", "start": 9, "excl_end": 10}, {"label": "arg_evaluative", "start": 10, "excl_end": 11}, {"label": "arg_request|experiment", "start": 11, "excl_end": 12}, {"label": "arg_fact", "start": 12, "excl_end": 13}, {"label": "arg_request|experiment", "start": 13, "excl_end": 14}, {"label": "arg_evaluative", "start": 14, "excl_end": 15}, {"label": "arg_structuring|heading", "start": 15, "excl_end": 16}, {"label": "arg_request|clarification", "start": 16, "excl_end": 17}, {"label": "arg_request|typo", "start": 17, "excl_end": 18}], "alignment": [{"label": "reb_idxs_9|10|11", "start": 7, "excl_end": 14}, {"label": "reb_idxs_22|23|24", "start": 16, "excl_end": 18}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 6}, {"label": "tt_segment_1", "start": 6, "excl_end": 15}, {"label": "tt_segment_2", "start": 15, "excl_end": 18}], "entity_grid": [{"label": "meta", "start": 0, "excl_end": 16}, {"label": "domain", "start": 0, "excl_end": 13}, {"label": "adaptation", "start": 0, "excl_end": 10}, {"label": "shift", "start": 0, "excl_end": 13}, {"label": "scenario", "start": 0, "excl_end": 4}, {"label": "learning", "start": 0, "excl_end": 8}, {"label": "model", "start": 1, "excl_end": 14}, {"label": "shot", "start": 1, "excl_end": 11}, {"label": "performance", "start": 1, "excl_end": 11}, {"label": "improvements", "start": 1, "excl_end": 5}, {"label": "problem", "start": 3, "excl_end": 14}, {"label": "paper", "start": 3, "excl_end": 12}, {"label": "omniglot", "start": 5, "excl_end": 13}, {"label": "approach", "start": 7, "excl_end": 13}, {"label": "revgrad", "start": 7, "excl_end": 9}, {"label": "baseline", "start": 7, "excl_end": 11}, {"label": "it", "start": 8, "excl_end": 16}, {"label": "i", "start": 10, "excl_end": 13}, {"label": "training", "start": 11, "excl_end": 16}, {"label": "figure", "start": 16, "excl_end": 17}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_structuring", "start": 0, "excl_end": 1}, {"label": "rebuttal_social", "start": 1, "excl_end": 2}, {"label": "rebuttal_done", "start": 2, "excl_end": 3}, {"label": "rebuttal_structuring", "start": 3, "excl_end": 4}, {"label": "rebuttal_answer", "start": 4, "excl_end": 8}, {"label": "rebuttal_structuring", "start": 8, "excl_end": 9}, {"label": "rebuttal_concede-criticism", "start": 9, "excl_end": 10}, {"label": "rebuttal_done", "start": 10, "excl_end": 11}, {"label": "rebuttal_structuring", "start": 11, "excl_end": 13}, {"label": "rebuttal_done", "start": 13, "excl_end": 14}, {"label": "rebuttal_structuring", "start": 14, "excl_end": 15}, {"label": "rebuttal_reject-request", "start": 15, "excl_end": 19}, {"label": "rebuttal_structuring", "start": 19, "excl_end": 20}, {"label": "rebuttal_reject-request", "start": 20, "excl_end": 21}, {"label": "rebuttal_structuring", "start": 21, "excl_end": 22}, {"label": "rebuttal_done", "start": 22, "excl_end": 23}, {"label": "rebuttal_answer", "start": 23, "excl_end": 25}, {"label": "rebuttal_other", "start": 25, "excl_end": 26}], "alignment": [{"label": "rev_idxs_7|8", "start": 3, "excl_end": 8}, {"label": "rev_idxs_8|9|10", "start": 9, "excl_end": 14}, {"label": "rev_idxs_11", "start": 14, "excl_end": 19}, {"label": "rev_idxs_12|13", "start": 19, "excl_end": 21}, {"label": "rev_idxs_16|17", "start": 22, "excl_end": 25}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 3}, {"label": "tt_segment_1", "start": 3, "excl_end": 12}, {"label": "tt_segment_2", "start": 12, "excl_end": 21}, {"label": "tt_segment_3", "start": 21, "excl_end": 26}], "entity_grid": [{"label": "we", "start": 1, "excl_end": 22}, {"label": "you", "start": 1, "excl_end": 9}, {"label": "work", "start": 1, "excl_end": 2}, {"label": "suggestions", "start": 2, "excl_end": 9}, {"label": "experiments", "start": 2, "excl_end": 8}, {"label": "concern", "start": 3, "excl_end": 8}, {"label": "meta", "start": 3, "excl_end": 23}, {"label": "revgrad", "start": 3, "excl_end": 4}, {"label": "feature", "start": 4, "excl_end": 7}, {"label": "invariance", "start": 4, "excl_end": 5}, {"label": "embedding", "start": 4, "excl_end": 7}, {"label": "domain", "start": 6, "excl_end": 25}, {"label": "adaptation", "start": 6, "excl_end": 25}, {"label": "data", "start": 6, "excl_end": 23}, {"label": "approach", "start": 7, "excl_end": 25}, {"label": "target", "start": 7, "excl_end": 17}, {"label": "baselines", "start": 12, "excl_end": 13}, {"label": "setting", "start": 13, "excl_end": 20}, {"label": "baseline", "start": 14, "excl_end": 15}, {"label": "training", "start": 14, "excl_end": 23}, {"label": "way", "start": 16, "excl_end": 18}, {"label": "test", "start": 17, "excl_end": 23}]}}, {"review_id": "r1x7498kcS", "review_sentences": ["This paper explores self-supervised learning in the low-data regime, comparing results to self-supervised learning on larger datasets.", "BiGAN, RotNet, and DeepCluster serve as the reference self-supervised methods.", "It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation.", "A performance gap exists for deeper layers, suggesting that larger datasets are required for self-supervised learning of useful filters in deeper network layers.\n\n", "I believe the primary claim of this paper is neither surprising nor novel.", "The long history of successful hand-designed descriptors in computer vision, such as SIFT [Lowe, 1999] and HOG [Dalal and Triggs, 2005], suggest that one can design (with no data at all) features reminiscent of those learned in the first couple layers of a convolutional neural network (local image gradients, followed by characterization of those gradients over larger local windows).\n\n", "More importantly, it is already well established that it is possible to learn, from only a few images, filter sets that resemble the early layers of filters learned by CNNs.", "This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.\n\n", "For example, see the following paper (over 5600 citations according to Google scholar):\n\n", "[1] Bruno A. Olshausen and David J. Field.", "Emergence of simple-cell receptive field properties by learning a sparse code for natural images.", "Nature, 1996.\n\n", "Figure 4 of [1] shows results for learning 16x16 filters using \"ten 512x512 images of natural scenes\".", "Compare to the conv1 filters in Figure 2 of the paper under review.", "This 1996 paper clearly established that it is possible to learn such filters from a small number of images.", "There is long history of sparse coding and dictionary learning techniques, including multilayer representations, that follows from the early work of [1].", "The paper should at minimum engage with this extensive history, and, in light of it, explain whether its claims are actually novel."], "rebuttal_sentences": ["We hope that the reviewer will change his opinion once we clarify the goal of our paper and explain how it relates to prior work, as we believe we are fundamentally on the same page.\n\n", "We are well aware of SIFT, HOG, the results of Olshausen and Field on learning image filters from a few example images (some of us are sufficiently old to have implemented all such methods from scratch as grad students!) and no annotations, as well as Mallat\u2019s Scattering nets [1].", "In fact, we discuss and evaluate Oyallon\u2019s 2017 implementation [2] of this at page 5 and table 2 in the paper.\n\n", "However, the existence of these methods does not detract from the message of this paper.", "Our goal is to provide \u201ccritical analysis\u201d of current self-supervision methods because these *specific* tools are now very heavily researched.", "Our paper sends a cautionary message: current self-supervised learning techniques cannot improve on what can be obtained from a single image plus transformations for early layers in a network, and only improves in a limited manner for deeper layers, despite ingesting millions of images (which is touted as their key advantage).", "In particular, the claims are not limited to the first few layers as we show that one image recovers two thirds of the performance of deeper layers as well.", "This message, which is a partially negative result, stands on its own, regardless of whether good low-level features can be obtained in some other ways (e.g. manually) and, we hope the reviewer will agree, should be known by the community.\n\n", "Nevertheless, we also agree with the reviewer that it is interesting to put these findings in a broader context, so we are happy to expand the discussion of prior feature learning/design work further.", "However, please note that none of this literature makes our specific findings on the limits of self-supervision obvious.", "Furthermore, although this is a little besides the point, in the paper we do show in Table 2 that scattering transforms works as well as conv1, but that from conv2 onwards self-supervision on a single image does better, so even the claim that handcrafted features are equivalent to the first few layers in deep networks is not proven.", "Also, the fact that Olshausens\u2019s filters resemble conv1 does not mean that they are equivalent to conv1 in recognition performance.\n\n", "\u2014\n", "[1] J. Bruna and S. Mallat. \"Invariant scattering convolution networks.\" TPAMI 2013\n", "[2] E. Oyallon, et al. \"Scaling the scattering transform: Deep hybrid networks.\" ICCV 2017"], "review_segmentations": {"label": [{"label": "arg_structuring|summary", "start": 0, "excl_end": 4}, {"label": "arg_evaluative", "start": 4, "excl_end": 5}, {"label": "arg_fact", "start": 5, "excl_end": 7}, {"label": "arg_evaluative", "start": 7, "excl_end": 8}, {"label": "arg_structuring|heading", "start": 8, "excl_end": 9}, {"label": "arg_other", "start": 9, "excl_end": 12}, {"label": "arg_fact", "start": 12, "excl_end": 16}, {"label": "arg_evaluative", "start": 16, "excl_end": 17}], "alignment": [{"label": "reb_idxs_1|2|3|4|5|6|7|8|9|10|11", "start": 5, "excl_end": 6}, {"label": "reb_idxs_1|2|3|4|5|6|7|8|9|10|11", "start": 9, "excl_end": 12}, {"label": "reb_idxs_1|2|3|4|5|6|7|8|9|10|11", "start": 16, "excl_end": 17}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 6}, {"label": "tt_segment_1", "start": 6, "excl_end": 12}, {"label": "tt_segment_2", "start": 12, "excl_end": 17}], "entity_grid": [{"label": "paper", "start": 0, "excl_end": 16}, {"label": "self", "start": 0, "excl_end": 3}, {"label": "learning", "start": 0, "excl_end": 15}, {"label": "data", "start": 0, "excl_end": 5}, {"label": "results", "start": 0, "excl_end": 12}, {"label": "datasets", "start": 0, "excl_end": 3}, {"label": "it", "start": 2, "excl_end": 16}, {"label": "layers", "start": 2, "excl_end": 6}, {"label": "network", "start": 2, "excl_end": 5}, {"label": "image", "start": 2, "excl_end": 5}, {"label": "filters", "start": 3, "excl_end": 14}, {"label": "history", "start": 5, "excl_end": 16}, {"label": "images", "start": 6, "excl_end": 14}, {"label": "field", "start": 9, "excl_end": 10}, {"label": "figure", "start": 12, "excl_end": 13}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 1}, {"label": "rebuttal_reject-criticism", "start": 1, "excl_end": 8}, {"label": "rebuttal_by-cr", "start": 8, "excl_end": 9}, {"label": "rebuttal_reject-criticism", "start": 9, "excl_end": 12}, {"label": "rebuttal_structuring", "start": 12, "excl_end": 13}, {"label": "rebuttal_other", "start": 13, "excl_end": 15}], "alignment": [{"label": "rev_idxs_5|9|10|11|16", "start": 1, "excl_end": 12}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 3}, {"label": "tt_segment_1", "start": 3, "excl_end": 8}, {"label": "tt_segment_2", "start": 8, "excl_end": 15}], "entity_grid": [{"label": "we", "start": 0, "excl_end": 10}, {"label": "reviewer", "start": 0, "excl_end": 8}, {"label": "goal", "start": 0, "excl_end": 4}, {"label": "paper", "start": 0, "excl_end": 10}, {"label": "it", "start": 0, "excl_end": 8}, {"label": "work", "start": 0, "excl_end": 8}, {"label": "page", "start": 0, "excl_end": 2}, {"label": "image", "start": 1, "excl_end": 10}, {"label": "filters", "start": 1, "excl_end": 11}, {"label": "images", "start": 1, "excl_end": 5}, {"label": "methods", "start": 1, "excl_end": 4}, {"label": "mallat", "start": 1, "excl_end": 13}, {"label": "fact", "start": 2, "excl_end": 11}, {"label": "oyallon", "start": 2, "excl_end": 14}, {"label": "table", "start": 2, "excl_end": 10}, {"label": "message", "start": 3, "excl_end": 7}, {"label": "self", "start": 4, "excl_end": 10}, {"label": "supervision", "start": 4, "excl_end": 10}, {"label": "learning", "start": 5, "excl_end": 8}, {"label": "layers", "start": 5, "excl_end": 10}, {"label": "performance", "start": 6, "excl_end": 11}, {"label": "features", "start": 7, "excl_end": 10}, {"label": "findings", "start": 8, "excl_end": 9}, {"label": "conv1", "start": 10, "excl_end": 11}, {"label": "networks", "start": 10, "excl_end": 14}]}}, {"review_id": "S1xAWnjRFH", "review_sentences": ["Summary:\n\n", "This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another.", "Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation.", "Authors argue that ME bias could help the model to handle new classes and rare events better.\n\n", "My comments:\n\n", "I very much enjoyed reading this paper.", "I support accepting this paper.", "It highlights one of the missing inductive biases in ML and proposes it as a challenge.", "As the authors also agree, ME bias is missing not just in DNNs.", "It is the issue of MLE.", "It would be good to have some non-NN results too.", "I see this is a challenge for MLE than DNNs.\n\n", "1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?\n", "2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.\n", "3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain.\n", "4. Are the authors willing to release the code and data to reproduce the results?\n\n", "Minor comments:\n\n\n", "1. Page 3: second para, line 4: \u201cour aim is to study\u201d\n", "2. Page 5: last line: estimate for -> estimated for\n", "3. Section 4.2: 3rd line: \u201cthe class for the from\u201d\n\n", "=====================================================\n\n", "After rebuttal: I have read the authors' response and  I stand by my decision."], "rebuttal_sentences": ["Thank you for your supportive review.\n", "We answer the specific queries below and have also added them to the revised version of the paper.\n", "1.         We found that the entropy regularizer produces an ME score that stays constant across training, at the cost of the model being less confident about predictions made for seen classes.", "We added details regarding this condition to the manuscript.\n", "2.         The base rate is the probability of observing a new word in the target at that particular point in training.", "We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word.", "Thus, the base rate at time t in training is defined as:\n", "$$P(\\text{new in target at t}) = \\frac{ \\text{# of unseen sentences in target with new words}} {\\text{# of unseen sentences}}$$\n", "3.", "In Section 4.2, we use \u201cnew\u201d to refer to the set of all the unseen classes at a particular timepoint t.", "For the classifier, P(N|t) is calculated by adding the probabilities the model assigns to all the \u201cnew\u201d classes when iterating through the remaining corpus (similar to Equation 1 in our paper).", "For the dataset, we compute P(N|t) by sampling all unseen images in the corpus and compute the proportion from \u201cnew\u201d classes given their ground truth labels.\n", "4.         We will release our code and data with the publication of the paper.", "Most of our experiments are easy to replicate as they use standard datasets, models, loss functions and optimizers.", "We sincerely hope that our challenge and these resources will stimulate progress in this area.\n\n", "Please also see above where we write a general response to all reviews."], "review_segmentations": {"label": [{"label": "arg_structuring|heading", "start": 0, "excl_end": 1}, {"label": "arg_structuring|summary", "start": 1, "excl_end": 4}, {"label": "arg_structuring|heading", "start": 4, "excl_end": 5}, {"label": "arg_social", "start": 5, "excl_end": 7}, {"label": "arg_evaluative", "start": 7, "excl_end": 8}, {"label": "arg_fact", "start": 8, "excl_end": 10}, {"label": "arg_evaluative", "start": 10, "excl_end": 11}, {"label": "arg_fact", "start": 11, "excl_end": 12}, {"label": "arg_request|explanation", "start": 12, "excl_end": 14}, {"label": "arg_evaluative", "start": 14, "excl_end": 15}, {"label": "arg_request|explanation", "start": 15, "excl_end": 16}, {"label": "arg_structuring|heading", "start": 16, "excl_end": 17}, {"label": "arg_request|edit", "start": 17, "excl_end": 20}, {"label": "arg_structuring|heading", "start": 20, "excl_end": 21}, {"label": "arg_social", "start": 21, "excl_end": 22}], "alignment": [{"label": "reb_idxs_2|3", "start": 12, "excl_end": 13}, {"label": "reb_idxs_4|5|6|7|8", "start": 13, "excl_end": 14}, {"label": "reb_idxs_9|10|11", "start": 14, "excl_end": 15}, {"label": "reb_idxs_12|13", "start": 16, "excl_end": 17}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 4}, {"label": "tt_segment_1", "start": 4, "excl_end": 16}, {"label": "tt_segment_2", "start": 16, "excl_end": 22}], "entity_grid": [{"label": "paper", "start": 1, "excl_end": 6}, {"label": "me", "start": 1, "excl_end": 12}, {"label": "bias", "start": 1, "excl_end": 8}, {"label": "object", "start": 1, "excl_end": 2}, {"label": "it", "start": 1, "excl_end": 13}, {"label": "authors", "start": 2, "excl_end": 21}, {"label": "comments", "start": 4, "excl_end": 16}, {"label": "i", "start": 5, "excl_end": 21}, {"label": "challenge", "start": 7, "excl_end": 11}, {"label": "dnns", "start": 8, "excl_end": 11}, {"label": "mle", "start": 9, "excl_end": 12}, {"label": "results", "start": 10, "excl_end": 15}, {"label": "figure", "start": 12, "excl_end": 13}, {"label": "you", "start": 12, "excl_end": 14}, {"label": "section", "start": 14, "excl_end": 19}, {"label": "class", "start": 14, "excl_end": 19}, {"label": "page", "start": 17, "excl_end": 18}, {"label": "line", "start": 17, "excl_end": 19}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 1}, {"label": "rebuttal_structuring", "start": 1, "excl_end": 2}, {"label": "rebuttal_answer", "start": 2, "excl_end": 3}, {"label": "rebuttal_done", "start": 3, "excl_end": 4}, {"label": "rebuttal_answer", "start": 4, "excl_end": 14}, {"label": "rebuttal_social", "start": 14, "excl_end": 16}], "alignment": [{"label": "rev_idxs_12", "start": 2, "excl_end": 4}, {"label": "rev_idxs_13", "start": 4, "excl_end": 9}, {"label": "rev_idxs_14", "start": 9, "excl_end": 12}, {"label": "rev_idxs_16", "start": 12, "excl_end": 14}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 15}, {"label": "tt_segment_1", "start": 15, "excl_end": 16}], "entity_grid": [{"label": "we", "start": 1, "excl_end": 15}, {"label": "paper", "start": 1, "excl_end": 12}, {"label": "training", "start": 2, "excl_end": 6}, {"label": "model", "start": 2, "excl_end": 10}, {"label": "classes", "start": 2, "excl_end": 11}, {"label": "base", "start": 4, "excl_end": 6}, {"label": "rate", "start": 4, "excl_end": 6}, {"label": "probability", "start": 4, "excl_end": 5}, {"label": "word", "start": 4, "excl_end": 5}, {"label": "target", "start": 4, "excl_end": 7}, {"label": "sentences", "start": 5, "excl_end": 7}, {"label": "corpus", "start": 5, "excl_end": 11}, {"label": "t", "start": 6, "excl_end": 11}, {"label": "p", "start": 7, "excl_end": 11}, {"label": "n", "start": 10, "excl_end": 11}]}}, {"review_id": "Skx7vDii3X", "review_sentences": ["This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.", "Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances.", "This is an interesting work.\n\n", "The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning.", "(2) The proposed approach produced effective empirical results.\n\n", "The drawbacks  of the work include the following: (1) There is not much technical contribution.", "It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning.", "Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.", "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "This is a major concern."], "rebuttal_sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.\n\n", "\"(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.\"\n\n", ">>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation.", "The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples.", "While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A).", "This leads to better generalization ability for test tasks.\n", "In Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as \"Label Propagation\").\n\n", "\"", "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "This is a major concern.\"\n\n", ">>> At first, we want to clarify the few-shot network architecture setting.", "Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]).", "Our method belongs to the first one, which contains much fewer layers than the ResNet setting.", "Thus, it is more reasonable to compare TADAM with ResNet version of our method.", "To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:\n\n", "Method", "1-shot    5-shot\n", "SNAIL [4]", "55.71     68.88\n", "adaResNet [5]                        56.88     71.94\n", "Discriminative k-shot [6]", "56.30     73.90\n", "TADAM [7]", "58.50     76.70\n", "--------------------------------------------------------\n", "Ours", "59.46     75.65\n", "--------------------------------------------------------\n\n", "It can be seen that we beat TADAM for 1-shot setting.", "For 5-shot, we outperform all other recent high-performance methods except for TADAM.\n\n", ">>> We want to clarify that \"Label Propagation\" in Table 1 and Table 2 is a strong baseline.", "It combines label propagation method [8] with episodic meta-learning.", "The usage of transductive inference makes this baseline outperform most published state-of-the-art methods.", "Moreover, the performance of TPN over label propagation is not very small.", "For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with \"Higher Shot\" training.", "The improvements are even larger for tieredImagenet with 4.68% and 2.87%.", "We believe in few-shot learning, this is a large improvement.\n\n\n", "[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.\n", "[2] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.\n", "[3] Yang, Flood Sung Yongxin et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.\n", "[4] Mishra, Nikhil et al. \"A simple neural attentive meta-learner.\" ICLR. 2018.\n", "[5] Munkhdalai, Tsendsuren et al. \"Rapid adaptation with conditionally shifted neurons.\" ICML. 2018.\n", "[6] Bauer, Matthias et al. \"Discriminative k-shot learning using probabilistic models.\" arXiv. 2017.\n", "[7] Oreshkin, B.N., Lacoste, A. and Rodriguez, P., 2018. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS. 2018.\n", "[8] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004."], "review_segmentations": {"label": [{"label": "arg_structuring|summary", "start": 0, "excl_end": 2}, {"label": "arg_evaluative", "start": 2, "excl_end": 6}, {"label": "arg_fact", "start": 6, "excl_end": 7}, {"label": "arg_evaluative", "start": 7, "excl_end": 8}, {"label": "arg_structuring|quote", "start": 8, "excl_end": 9}, {"label": "arg_evaluative", "start": 9, "excl_end": 10}, {"label": "arg_fact", "start": 10, "excl_end": 12}], "alignment": [{"label": "reb_idxs_2|3|4|5|6|7", "start": 5, "excl_end": 8}, {"label": "reb_idxs_8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|38|39", "start": 8, "excl_end": 12}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 3}, {"label": "tt_segment_1", "start": 3, "excl_end": 5}, {"label": "tt_segment_2", "start": 5, "excl_end": 12}], "entity_grid": [{"label": "paper", "start": 0, "excl_end": 9}, {"label": "shot", "start": 0, "excl_end": 10}, {"label": "learning", "start": 0, "excl_end": 7}, {"label": "label", "start": 0, "excl_end": 8}, {"label": "propagation", "start": 0, "excl_end": 8}, {"label": "end", "start": 0, "excl_end": 6}, {"label": "work", "start": 2, "excl_end": 10}, {"label": "it", "start": 3, "excl_end": 8}, {"label": "results", "start": 4, "excl_end": 10}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_structuring", "start": 0, "excl_end": 3}, {"label": "rebuttal_reject-criticism", "start": 3, "excl_end": 8}, {"label": "rebuttal_structuring", "start": 8, "excl_end": 13}, {"label": "rebuttal_reject-criticism", "start": 13, "excl_end": 18}, {"label": "rebuttal_summary", "start": 18, "excl_end": 27}, {"label": "rebuttal_structuring", "start": 27, "excl_end": 28}, {"label": "rebuttal_summary", "start": 28, "excl_end": 30}, {"label": "rebuttal_structuring", "start": 30, "excl_end": 31}, {"label": "rebuttal_reject-criticism", "start": 31, "excl_end": 40}, {"label": "rebuttal_other", "start": 40, "excl_end": 48}], "alignment": [{"label": "rev_idxs_5|6|7", "start": 2, "excl_end": 8}, {"label": "rev_idxs_8|9|10|11", "start": 8, "excl_end": 40}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 2}, {"label": "tt_segment_1", "start": 2, "excl_end": 8}, {"label": "tt_segment_2", "start": 8, "excl_end": 18}, {"label": "tt_segment_3", "start": 18, "excl_end": 33}, {"label": "tt_segment_4", "start": 33, "excl_end": 48}], "entity_grid": null}}, {"review_id": "SJgAEEpDhQ", "review_sentences": ["Summary:\n\n", "This paper addresses the computational aspects of Viterbi-based encoding for neural networks.\n\n", "In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis.", "Now consider a codebook with n convolutional codes, of rate 1/k.", "Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits.", "Then the memory footprint (in terms of messages) is reduced by rate k/n.", "This is the format that will be used to encode the row indices in a matrix, with n columns.", "(The value of each nonzero is stored separately.)", "However, it is clear that not all messages are possible, only those in the \"range space\" of my codes. (This part is previous work Lee 2018.)\n\n", "The \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves.", "A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task.", "Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords.\n\n", "Pros:\n", "- I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance.\n", "- The idea is theoretically sound and interesting.\n\n", "Cons:\n", "- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "Compressability is evaluated, but that was already present in the previous work.", "Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.\n", "- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.\n", "- Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)"], "rebuttal_sentences": ["Thank you very much for the positive comments.", "We added the more experimental data of runtime analysis to address the Reviewer's main concern.\n\n", "Q1. My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "Compressability is evaluated, but that was already present in the previous work.", "Therefore the novel contribution of this paper over [1] is not clearly outlined.\n\n", "We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [1].", "We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles.", "The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.", "We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [1].", "The proposed method outperformed both baseline method and [1] in all simulation results.", "Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.", "While preparing for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.\n\n", "Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.\n\n", "In the revision, we added the more precise mathematical description of the input and output of each block in Figure 1 and showed the change of the exact weight representation at each process.", "We first prune weights in a neural network with the Viterbi-based pruning scheme [1], then we quantize the pruned weights with the alternating quantization method [2].", "Our main contribution is the third process, which includes encoding each weight with the Viterbi algorithm, and retraining for the recovery of accuracy.", "With our proposed method, the sparse and encoded weights are reconstructed to a dense matrix as described in Figure 2.", "Figure 2 illustrates the purpose of our proposed scheme, which is the parallelization of the whole sparse-to-dense conversion process with the VDs while maintaining the high compression rate.\n\n", "Q3. Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)\n\n", "Thanks very much for the suggestions.", "We tried to fix grammatical mistakes as much as possible in the revision.\n\n", "Reference\n", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.\n", "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "review_segmentations": {"label": [{"label": "arg_structuring|heading", "start": 0, "excl_end": 1}, {"label": "arg_structuring|summary", "start": 1, "excl_end": 12}, {"label": "arg_structuring|heading", "start": 12, "excl_end": 13}, {"label": "arg_evaluative", "start": 13, "excl_end": 15}, {"label": "arg_structuring|heading", "start": 15, "excl_end": 16}, {"label": "arg_evaluative", "start": 16, "excl_end": 20}, {"label": "arg_request|typo", "start": 20, "excl_end": 21}], "alignment": [{"label": "reb_idxs_5|6|7|8|9|10|11|12|13", "start": 16, "excl_end": 19}, {"label": "reb_idxs_14|15|16|17|18|19", "start": 19, "excl_end": 20}, {"label": "reb_idxs_20|21|22", "start": 20, "excl_end": 21}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 9}, {"label": "tt_segment_1", "start": 9, "excl_end": 12}, {"label": "tt_segment_2", "start": 12, "excl_end": 15}, {"label": "tt_segment_3", "start": 15, "excl_end": 21}], "entity_grid": [{"label": "paper", "start": 1, "excl_end": 19}, {"label": "viterbi", "start": 1, "excl_end": 16}, {"label": "codes", "start": 2, "excl_end": 8}, {"label": "messages", "start": 2, "excl_end": 8}, {"label": "n", "start": 3, "excl_end": 6}, {"label": "rate", "start": 3, "excl_end": 5}, {"label": "k", "start": 4, "excl_end": 5}, {"label": "terms", "start": 5, "excl_end": 20}, {"label": "row", "start": 6, "excl_end": 11}, {"label": "value", "start": 7, "excl_end": 10}, {"label": "nonzero", "start": 7, "excl_end": 10}, {"label": "it", "start": 8, "excl_end": 19}, {"label": "work", "start": 8, "excl_end": 17}, {"label": "lee", "start": 8, "excl_end": 18}, {"label": "contribution", "start": 9, "excl_end": 18}, {"label": "i", "start": 13, "excl_end": 19}, {"label": "idea", "start": 14, "excl_end": 19}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 1}, {"label": "rebuttal_done", "start": 1, "excl_end": 2}, {"label": "rebuttal_structuring", "start": 2, "excl_end": 5}, {"label": "rebuttal_done", "start": 5, "excl_end": 6}, {"label": "rebuttal_summary", "start": 6, "excl_end": 7}, {"label": "rebuttal_done", "start": 7, "excl_end": 8}, {"label": "rebuttal_summary", "start": 8, "excl_end": 13}, {"label": "rebuttal_done", "start": 13, "excl_end": 14}, {"label": "rebuttal_structuring", "start": 14, "excl_end": 15}, {"label": "rebuttal_done", "start": 15, "excl_end": 16}, {"label": "rebuttal_summary", "start": 16, "excl_end": 20}, {"label": "rebuttal_structuring", "start": 20, "excl_end": 21}, {"label": "rebuttal_concede-criticism", "start": 21, "excl_end": 22}, {"label": "rebuttal_done", "start": 22, "excl_end": 23}, {"label": "rebuttal_structuring", "start": 23, "excl_end": 24}, {"label": "rebuttal_other", "start": 24, "excl_end": 26}], "alignment": [{"label": "rev_idxs_16", "start": 1, "excl_end": 3}, {"label": "rev_idxs_17", "start": 3, "excl_end": 4}, {"label": "rev_idxs_18", "start": 4, "excl_end": 14}, {"label": "rev_idxs_19", "start": 14, "excl_end": 20}, {"label": "rev_idxs_20", "start": 20, "excl_end": 23}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 2}, {"label": "tt_segment_1", "start": 2, "excl_end": 5}, {"label": "tt_segment_2", "start": 5, "excl_end": 14}, {"label": "tt_segment_3", "start": 14, "excl_end": 15}, {"label": "tt_segment_4", "start": 15, "excl_end": 20}, {"label": "tt_segment_5", "start": 20, "excl_end": 26}], "entity_grid": null}}, {"review_id": "H1x3aUom2X", "review_sentences": ["This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training.", "The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds.", "However, the experimental results are weak in justifying the paper's claims.\n\n", "Pros:\n", "* The problem is interesting and well explained\n", "* The proposed method is clearly motivated\n", "* The proposal looks theoretically solid\n\n", "Cons:\n\n", "* It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.", "There is no direct comparison of performance.\n\n", "* Fig. 3 needs more explanation.", "The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation.", "Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.\n\n", "* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\n\n", "* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.", "Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.\n\n", "A typo in page 6, last line: wth -> with"], "rebuttal_sentences": ["We thank Reviewer 2 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in the review:\n\n", "1. \u201cIt is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\u201d\n\n", "We do not claim that our method is more efficient than Miyato et al.\u2019s method, which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation.", "In fact, our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al.\u2019s.\n\n", "We introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al.\u2019s approximation.", "Therefore, Miyato et al.\u2019s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN\u2019s generalization performance (please see our generalization bounds in Section 3).", "To further support our argument, we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers, resulting in better generalization and test performance.", "The results are presented in Appendix A.1.", "Furthermore, we run several experiments to show that our method is not significantly slower than Miyato et al.\u2019s method, and we report the results in Appendix A.1, Table 3.\n\n", "2. \u201cFig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing\u201d\n\n", "We relabel the axes and add a more thorough explanation in the caption.", "We note that the text explaining Figure 3 mentions how the margin normalization is performed (paragraph 3 in section 5.1): the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4.", "We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset.\n\n", "3. \u201cThe epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\u201d\n\n", "Yes, the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks.", "This is because the two norms can behave very differently in adversarial attack experiments.", "For example, a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5.", "On the other hand, a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5, resulting in a much less powerful attack.", "Based on this comment, we update the plots with the same attack-norm to have the same scale.\n\n", "4. \"Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.\"\n\n", "We redo the visualization in Figure 6 to make the gains provided by SN clearer.", "We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.\n\n", "5. \"The baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "It is thus unclear whether the advantage can be maintained after applying these standard regularisers.\"\n\n", "We did not originally discuss weight decay, dropout, and batch normalization as none of these methods were motivated by the theory we introduced in section 3.", "However, due to the reviewers\u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2.", "In our experiments, the SN-regularized network still performs better in terms of test accuracy."], "review_segmentations": {"label": [{"label": "arg_evaluative", "start": 0, "excl_end": 3}, {"label": "arg_structuring|heading", "start": 3, "excl_end": 4}, {"label": "arg_evaluative", "start": 4, "excl_end": 7}, {"label": "arg_structuring|heading", "start": 7, "excl_end": 8}, {"label": "arg_evaluative", "start": 8, "excl_end": 10}, {"label": "arg_request|explanation", "start": 10, "excl_end": 11}, {"label": "arg_request|edit", "start": 11, "excl_end": 13}, {"label": "arg_request|explanation", "start": 13, "excl_end": 14}, {"label": "arg_evaluative", "start": 14, "excl_end": 18}, {"label": "arg_request|typo", "start": 18, "excl_end": 19}], "alignment": [{"label": "reb_idxs_2|3|4|5|6|7|8|9", "start": 8, "excl_end": 10}, {"label": "reb_idxs_10|11|12|13", "start": 10, "excl_end": 13}, {"label": "reb_idxs_14|15|16|17|18|19", "start": 13, "excl_end": 14}, {"label": "reb_idxs_20|21|22|23", "start": 14, "excl_end": 18}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 7}, {"label": "tt_segment_1", "start": 7, "excl_end": 13}, {"label": "tt_segment_2", "start": 13, "excl_end": 19}], "entity_grid": [{"label": "paper", "start": 0, "excl_end": 2}, {"label": "problem", "start": 0, "excl_end": 14}, {"label": "generalisation", "start": 0, "excl_end": 14}, {"label": "proposal", "start": 1, "excl_end": 6}, {"label": "sn", "start": 1, "excl_end": 11}, {"label": "margin", "start": 1, "excl_end": 11}, {"label": "results", "start": 2, "excl_end": 15}, {"label": "method", "start": 5, "excl_end": 8}, {"label": "it", "start": 8, "excl_end": 17}, {"label": "fig.", "start": 10, "excl_end": 14}, {"label": "explanation", "start": 10, "excl_end": 11}, {"label": "advantage", "start": 15, "excl_end": 17}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 1}, {"label": "rebuttal_structuring", "start": 1, "excl_end": 3}, {"label": "rebuttal_contradict-assertion", "start": 3, "excl_end": 8}, {"label": "rebuttal_done", "start": 8, "excl_end": 9}, {"label": "rebuttal_reject-criticism", "start": 9, "excl_end": 10}, {"label": "rebuttal_structuring", "start": 10, "excl_end": 11}, {"label": "rebuttal_done", "start": 11, "excl_end": 12}, {"label": "rebuttal_answer", "start": 12, "excl_end": 14}, {"label": "rebuttal_structuring", "start": 14, "excl_end": 15}, {"label": "rebuttal_answer", "start": 15, "excl_end": 20}, {"label": "rebuttal_structuring", "start": 20, "excl_end": 22}, {"label": "rebuttal_done", "start": 22, "excl_end": 24}, {"label": "rebuttal_structuring", "start": 24, "excl_end": 26}, {"label": "rebuttal_answer", "start": 26, "excl_end": 27}, {"label": "rebuttal_done", "start": 27, "excl_end": 28}, {"label": "rebuttal_answer", "start": 28, "excl_end": 29}], "alignment": [{"label": "rev_idxs_8|9", "start": 2, "excl_end": 10}, {"label": "rev_idxs_10|11|12", "start": 10, "excl_end": 14}, {"label": "rev_idxs_13", "start": 14, "excl_end": 20}, {"label": "rev_idxs_16|17", "start": 20, "excl_end": 29}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 3}, {"label": "tt_segment_1", "start": 3, "excl_end": 5}, {"label": "tt_segment_2", "start": 5, "excl_end": 10}, {"label": "tt_segment_3", "start": 10, "excl_end": 11}, {"label": "tt_segment_4", "start": 11, "excl_end": 20}, {"label": "tt_segment_5", "start": 20, "excl_end": 29}], "entity_grid": null}}, {"review_id": "ryxWDI_Gsm", "review_sentences": ["The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule.", "In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces.", "Experiments are performaed on 2 simple toy datasets and a simple language modeling task.", "A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this.", "Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.\n\n", "Overall I like the motivation, provided background information and simplicity of the approach.", "Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation.", "However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.", "Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.", "Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.\n\n\n", "Strengths:\n", "- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step\n", "- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation\n", "- maze navigation shows incremental benefits over non-modulated plasticity\n", "- thorough experimentation\n", "- clipping-trick is a neat observation\n\n\n", "Weaknesses:\n", "- evaluation: only on toy tasks (which includes PTB), no real world tasks\n", "- very incremental improvements on PTB over a very simple baseline (far from SotA)\n", "- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures\n", "- no qualitative analysis on how modulation is actually use by the systems.", "E.g., when is modulation strong and when is it not used\n\n\n", "Comments:\n", "- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "Furthermore PTB is not a \"challenging\" LM benchmark."], "rebuttal_sentences": ["Thank you to Reviewer 3 for your thoughtful critique and we are happy that you share our enthusiasm for the motivation behind our approach.", "We share your curiosity on the qualitative behavior of such systems, and as documented in this response we have augmented the paper to address that and other of your suggestions.\n\n", "Re: \"- no qualitative analysis on how modulation is actually use by the systems.", "E.g., when is modulation strong and when is it not used \"\n\n", "Following the reviewer\u2019s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).", "This figure shows that while neuromodulation clearly reacts to reward, this reaction is complex and varies both within each episode and between runs.\n\n", "Re: \"- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\".", "Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "Furthermore PTB is not a \"challenging\" LM benchmark.\"\n\n", "We agree that, while the differences are statistically significant, they are minor.", "We were using that word technically, but do not want to give the wrong impression.", "We have thus modified the text to make it clear that we mean \u201cstatistically significant\u201d only.", "We also removed the adjective \u201cchallenging\u201d as regards PTB.\n\n", "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "We will keep trying to investigate such massive architectures in the future.\n\n\n", "Importantly, our purpose in this task is to show that, **all other things being equal**, a neuromodulated plastic LSTM can outperform a standard LSTM in realistic settings.", "We believe that outperforming standard LSTMs (again, all else being equal) on their \u201cworkhorse\u201d task domain (language processing) is worthy of notice, especially given the ease of implementation of our method which requires only adding a few lines of codes (<10) to a standard LSTM implementation and can then be used as a drop-in replacement to standard LSTM."], "review_segmentations": {"label": [{"label": "arg_structuring|summary", "start": 0, "excl_end": 3}, {"label": "arg_evaluative", "start": 3, "excl_end": 9}, {"label": "arg_social", "start": 9, "excl_end": 10}, {"label": "arg_structuring|heading", "start": 10, "excl_end": 11}, {"label": "arg_evaluative", "start": 11, "excl_end": 16}, {"label": "arg_structuring|heading", "start": 16, "excl_end": 17}, {"label": "arg_evaluative", "start": 17, "excl_end": 21}, {"label": "arg_fact", "start": 21, "excl_end": 22}, {"label": "arg_structuring|heading", "start": 22, "excl_end": 23}, {"label": "arg_evaluative", "start": 23, "excl_end": 25}], "alignment": [{"label": "reb_idxs_2|3|4|5", "start": 8, "excl_end": 10}, {"label": "reb_idxs_13|14|15|16|17", "start": 18, "excl_end": 20}, {"label": "reb_idxs_2|3|4|5", "start": 20, "excl_end": 22}, {"label": "reb_idxs_8|9|10|11|12", "start": 23, "excl_end": 25}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 10}, {"label": "tt_segment_1", "start": 10, "excl_end": 22}, {"label": "tt_segment_2", "start": 22, "excl_end": 25}], "entity_grid": [{"label": "paper", "start": 0, "excl_end": 23}, {"label": "work", "start": 0, "excl_end": 11}, {"label": "neuro", "start": 0, "excl_end": 12}, {"label": "modulation", "start": 0, "excl_end": 21}, {"label": "authors", "start": 1, "excl_end": 7}, {"label": "plasticity", "start": 1, "excl_end": 23}, {"label": "experiments", "start": 2, "excl_end": 9}, {"label": "toy", "start": 2, "excl_end": 17}, {"label": "language", "start": 2, "excl_end": 4}, {"label": "modeling", "start": 2, "excl_end": 4}, {"label": "task", "start": 2, "excl_end": 4}, {"label": "cue", "start": 3, "excl_end": 12}, {"label": "reward", "start": 3, "excl_end": 12}, {"label": "limitations", "start": 3, "excl_end": 12}, {"label": "improvements", "start": 4, "excl_end": 23}, {"label": "maze", "start": 4, "excl_end": 13}, {"label": "navigation", "start": 4, "excl_end": 13}, {"label": "i", "start": 5, "excl_end": 9}, {"label": "motivation", "start": 5, "excl_end": 11}, {"label": "simplicity", "start": 5, "excl_end": 7}, {"label": "experiment", "start": 6, "excl_end": 12}, {"label": "tasks", "start": 7, "excl_end": 17}, {"label": "evaluation", "start": 7, "excl_end": 17}, {"label": "analysis", "start": 8, "excl_end": 20}, {"label": "models", "start": 8, "excl_end": 23}, {"label": "benefits", "start": 12, "excl_end": 13}, {"label": "ptb", "start": 17, "excl_end": 24}, {"label": "baseline", "start": 18, "excl_end": 23}, {"label": "sota", "start": 18, "excl_end": 19}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 2}, {"label": "rebuttal_structuring", "start": 2, "excl_end": 9}, {"label": "rebuttal_mitigate-criticism", "start": 9, "excl_end": 11}, {"label": "rebuttal_done", "start": 11, "excl_end": 13}, {"label": "rebuttal_concede-criticism", "start": 13, "excl_end": 14}, {"label": "rebuttal_answer", "start": 14, "excl_end": 15}, {"label": "rebuttal_future", "start": 15, "excl_end": 16}, {"label": "rebuttal_answer", "start": 16, "excl_end": 18}], "alignment": [{"label": "rev_idxs_8|9|20|21", "start": 2, "excl_end": 6}, {"label": "rev_idxs_23", "start": 6, "excl_end": 13}, {"label": "rev_idxs_18|19", "start": 13, "excl_end": 18}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 4}, {"label": "tt_segment_1", "start": 4, "excl_end": 13}, {"label": "tt_segment_2", "start": 13, "excl_end": 18}], "entity_grid": [{"label": "reviewer", "start": 0, "excl_end": 14}, {"label": "we", "start": 0, "excl_end": 17}, {"label": "systems", "start": 1, "excl_end": 2}, {"label": "response", "start": 1, "excl_end": 14}, {"label": "paper", "start": 1, "excl_end": 6}, {"label": "modulation", "start": 2, "excl_end": 3}, {"label": "it", "start": 3, "excl_end": 11}, {"label": "figure", "start": 4, "excl_end": 5}, {"label": "neuromodulation", "start": 4, "excl_end": 5}, {"label": "task", "start": 4, "excl_end": 17}, {"label": "improvements", "start": 6, "excl_end": 7}, {"label": "they", "start": 7, "excl_end": 9}, {"label": "ptb", "start": 8, "excl_end": 12}, {"label": "architectures", "start": 13, "excl_end": 15}, {"label": "lstm", "start": 16, "excl_end": 17}]}}, {"review_id": "HyeHzlJ537", "review_sentences": ["This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough.", "The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection.", "On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.\n", "The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.", "Also, the writing can be improved by making the writing more concise and formal (examples of informal: \"spoil the network\", \"model is spoiled\", \"problem of increased classes\", \"many recent researches have been conducted\", \"lots of things to consider for training\", \"supervised learning was trained\" etc.).", "The contributions of the method could also be underlined more clearly in the abstract and introduction.", "The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.\n", "The idea of selective sampling for self-training is promising and the investigated questions are interesting.", "As far as I understand, the main contribution of this paper is the use of separate \"selection network\" to estimate the confidence of predictions by \"classification network\".", "However, as the \"selection network\" uses exactly the same input as \"classification network\", it is hard to imagine how it can learn additional information.", "For example, imagine the case of binary classification.", "If the selection network predicts 0 in come cases, it can be used to improve the result of \"classification network\" by flipping the corresponding label.", "How can you interpret such a thought experiment?", "One could understand the use of \"selection network\" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of \"selection network\" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.", "Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of \"classification network\" is greater than some threshold?", "Finally, could you show a plot of top-1 prediction of \"classification network\" vs score of \"selection network\" and elaborate on that?\n", "Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes.", "Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including \"selection network\" with threshold) is not very principled.", "Ablation study shows that the use of the \"selection network\" strategy does not improve the results without these heuristics.", "It would be interesting to see how these heuristics would do without \"selection network\", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG.", "In the current form of evaluation, it is hard to say if there is any benefit of using the \"selection network\" that is the main novelty of the paper.\n", "It is very valuable that the experimental results include many recently proposed methods.", "Besides, the settings are described in details that could help for the reproducibility of the results.", "However, I have a few concerns about the results.", "First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).", "Besides, as the base classifier is different for various baselines, it is hard to compare the methods.", "Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).", "How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice?", "Another important parameters is the number of iterations of the algorithm.", "How was it chosen?", "Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes?", "What would happen if you use random class splits or split animal classes (like in a more realistic scenario)?\n", "To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.\n\n", "Some questions and comments:\n", "- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\n", "- In the training procedure of \"selection network\" of Sections 3.1, do you use the same datapoints to train a \"classification network\" and \"selection network\"? If it is the case, how do you insure that the \"classification network\" does not learn to fit the data perfectly and thus all labels s_i are 1?\n", "- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?\n", "- What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?\n", "- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.\n", "- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?\n", "- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\n", "- Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?\n", "- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?"], "rebuttal_sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.\n\n\n", "Remark 1. Expression and detail\n\n", "A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.\n\n\n", "Remark 2.", "What is \"Selection Network\"?\n\n", "A : It is a module that estimates the confidence of the softmax output according to the inputs of the classification network.", "The selection network is trained with sigmoid and binary cross-entropy in a supervised manner.", "And the threshold is not 0.5 but high because selection network is learned with many \u20191\u2019 labels with close to 100 % training accuracy.\n", "The selection network has advantages in out of class unlabeled data.", "Since softmax output is a relative value, the softmax output can be high for some out of class unlabeled data.", "In our original paper (in table 10), there already exist results of softmax output for in or out of class unlabeled data with 0.9999 thresholds.", "Further, we experimented with the same threshold in table 4 of the new version and the results have shown that out of class unlabeled data are added even with an extremely small threshold such as 0.99999 (epsilon = 10^-5).\n\n\n", "Remark 3. \"As the base classifier is different for various baselines, it is hard to compare the methods.\"\n\n", "A : SST has a network structure similar to other papers.", "The difference of structure was that the selection network is added and Gaussian noise and the mean only batch norm are not used.", "As mentioned in the paper (4. Experiments), our supervised learning performs slightly better than conventional SSL algorithms because of different settings such as learning rate and Gaussian noise on the input layer.", "(When SST uses Gaussian noise, ours are also degraded.)\n\n\n", "Remark 4.", "Experiments Detail ( data setting, threshold, number of iterations, animal vs nonanimal)\n\n", "A :\n", "==> Data setting\n", "The purpose of experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "Therefore, we experimented with the popular setting.", "We have added a detailed description on the data setting to Section 6.3 of the supplementary material.\n\n\n", "==> Iterations & Threshold\n", "We have missed out on a detailed description of how to set up some hyper-parameters.", "We set parameters as follows.", "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.\n\n", "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "Other details are the same as those of the first experiment.", "(In previous versions, the training iterations of fixed mode had been fixed.", "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)\n\n\n", "=", "=> Animal vs non-animal\n", "The citation of that part is obscure and has been modified.", "We experimented similar to the [1] and they categorized according to the animal.", "Our approach is similar but not identical.", "Their unlabeled data came from only in 4 classes, however, we selected unlabeled data in all classes.\n", "[1] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018)\n\n\n", "Some Questions and comments\n\n", "Remark 5. \"The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\"\n\n", "A : To the best of our knowledge, the main purpose of transfer learning is to improve the performance on the target domain by effectively utilizing the knowledge of the source domain.", "However, in our case, there is no separated source and target domains.", "We focus on the single classification task.", "We think that the goal of our method and that of transfer learning are quite different.\n\n\n", "Remark 6. \"What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?\"\n\n", "A \" We have modified that expression and we wanted to address that \"if one class dominates the dataset, the performances are degraded by the imbalanced distribution.", "(Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches, 2013)\"\n\n\n", "Remark 7. \"Figure 3: wouldn\u2019t the plot of accuracy vs amount of data be more suitable here?\"\n\n", "A : I agree that your suggestion is more suitable for the figure.", "However, it is difficult to show the figure you want because the number of selected samples is different every time.\n\n\n", "Remark 8. \"Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\"\n\n", "A : The performance depends on the initial points, therefore sometimes the performance is not good. Since the inputs are the x and y coordinate values, it can be very easy to add to the training set.", "(ex.. class 1 : (-1, 0), (1, 0) , class 2 : (0.5, -0.5), (1.5, -0.5) , then decision boundary could be (:, -0.25) then class 2 unlabeled data (0, 0.5) is classified as class 1 and can have a very high selection score.)\n\n\n", "Remark 9. Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?\n\n", "A : In fixed mode, we ensemble the selection scores, which makes the prediction more consistent.", "Also, for a more reliable selection score, we do not add unlabeled data to the new training set and train with labeled data only for 5 iterations.\n\n\n", "Remark 10. \"How was it possible to improve the performance in the experiment of section 4.2 with 100% of irrelevant classes?\"\n\n", "A : We suspect that this performance improvement is due to re-initializing learning rate.", "After constructing a new training dataset, we retrain our model with the learning rate of the initial value.", "In decay mode (Figure 2, Figure 3 (a) and (b) of the original manuscript), the accuracy is slightly increased and gets saturated while unlabeled data is not being added.", "However, the accuracy begins to increase or decrease relatively more after adding selected data to the new training dataset.", "In fixed mode (Figure 3 (c) and (d) of the original manuscript), the improvement with the 100% of irrelevant classes seems to be due to re-initializing learning rate.", "However, SST algorithm with other ratios of out-of-class samples results in performance improvement compared to the 100% because out-of-class samples are not selected."], "review_segmentations": {"label": [{"label": "arg_structuring|summary", "start": 0, "excl_end": 1}, {"label": "arg_fact", "start": 1, "excl_end": 3}, {"label": "arg_evaluative", "start": 3, "excl_end": 4}, {"label": "arg_request|edit", "start": 4, "excl_end": 6}, {"label": "arg_evaluative", "start": 6, "excl_end": 10}, {"label": "arg_other", "start": 10, "excl_end": 11}, {"label": "arg_fact", "start": 11, "excl_end": 12}, {"label": "arg_request|explanation", "start": 12, "excl_end": 13}, {"label": "arg_evaluative", "start": 13, "excl_end": 14}, {"label": "arg_request|explanation", "start": 14, "excl_end": 15}, {"label": "arg_request|experiment", "start": 15, "excl_end": 16}, {"label": "arg_fact", "start": 16, "excl_end": 17}, {"label": "arg_evaluative", "start": 17, "excl_end": 19}, {"label": "arg_request|experiment", "start": 19, "excl_end": 20}, {"label": "arg_evaluative", "start": 20, "excl_end": 27}, {"label": "arg_request|explanation", "start": 27, "excl_end": 28}, {"label": "arg_request|clarification", "start": 28, "excl_end": 30}, {"label": "arg_request|explanation", "start": 30, "excl_end": 32}, {"label": "arg_structuring|summary", "start": 32, "excl_end": 33}, {"label": "arg_structuring|heading", "start": 33, "excl_end": 34}, {"label": "arg_request|explanation", "start": 34, "excl_end": 38}, {"label": "arg_request|edit", "start": 38, "excl_end": 39}, {"label": "arg_request|explanation", "start": 39, "excl_end": 40}, {"label": "arg_request|result", "start": 40, "excl_end": 41}, {"label": "arg_request|explanation", "start": 41, "excl_end": 43}], "alignment": [{"label": "reb_idxs_1|2", "start": 3, "excl_end": 4}, {"label": "reb_idxs_4|5|6|7|8|9|10|11", "start": 8, "excl_end": 10}, {"label": "reb_idxs_11", "start": 11, "excl_end": 16}, {"label": "reb_idxs_15|16", "start": 25, "excl_end": 32}, {"label": "reb_idxs_50", "start": 33, "excl_end": 34}, {"label": "reb_idxs_51|52|53|54|55", "start": 34, "excl_end": 35}, {"label": "reb_idxs_56|57|58", "start": 37, "excl_end": 38}, {"label": "reb_idxs_59|60|61", "start": 39, "excl_end": 40}, {"label": "reb_idxs_62|63|64", "start": 40, "excl_end": 41}, {"label": "reb_idxs_65|66|67", "start": 41, "excl_end": 42}, {"label": "reb_idxs_68|69|70|71|72|73|74", "start": 42, "excl_end": 43}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 33}, {"label": "tt_segment_1", "start": 33, "excl_end": 43}], "entity_grid": null}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 1}, {"label": "rebuttal_structuring", "start": 1, "excl_end": 2}, {"label": "rebuttal_concede-criticism", "start": 2, "excl_end": 3}, {"label": "rebuttal_structuring", "start": 3, "excl_end": 5}, {"label": "rebuttal_answer", "start": 5, "excl_end": 12}, {"label": "rebuttal_structuring", "start": 12, "excl_end": 13}, {"label": "rebuttal_answer", "start": 13, "excl_end": 17}, {"label": "rebuttal_structuring", "start": 17, "excl_end": 18}, {"label": "rebuttal_answer", "start": 18, "excl_end": 23}, {"label": "rebuttal_done", "start": 23, "excl_end": 24}, {"label": "rebuttal_structuring", "start": 24, "excl_end": 25}, {"label": "rebuttal_concede-criticism", "start": 25, "excl_end": 26}, {"label": "rebuttal_answer", "start": 26, "excl_end": 45}, {"label": "rebuttal_done", "start": 45, "excl_end": 46}, {"label": "rebuttal_answer", "start": 46, "excl_end": 49}, {"label": "rebuttal_structuring", "start": 49, "excl_end": 52}, {"label": "rebuttal_answer", "start": 52, "excl_end": 56}, {"label": "rebuttal_structuring", "start": 56, "excl_end": 57}, {"label": "rebuttal_answer", "start": 57, "excl_end": 59}, {"label": "rebuttal_structuring", "start": 59, "excl_end": 60}, {"label": "rebuttal_done", "start": 60, "excl_end": 61}, {"label": "rebuttal_reject-request", "start": 61, "excl_end": 62}, {"label": "rebuttal_structuring", "start": 62, "excl_end": 63}, {"label": "rebuttal_answer", "start": 63, "excl_end": 65}, {"label": "rebuttal_structuring", "start": 65, "excl_end": 66}, {"label": "rebuttal_answer", "start": 66, "excl_end": 68}, {"label": "rebuttal_structuring", "start": 68, "excl_end": 71}, {"label": "rebuttal_answer", "start": 71, "excl_end": 75}], "alignment": [{"label": "rev_idxs_3", "start": 1, "excl_end": 3}, {"label": "rev_idxs_8|9", "start": 4, "excl_end": 12}, {"label": "rev_idxs_25|26", "start": 12, "excl_end": 17}, {"label": "rev_idxs_26|27|28|29|30|31", "start": 18, "excl_end": 50}, {"label": "rev_idxs_33", "start": 50, "excl_end": 51}, {"label": "rev_idxs_34", "start": 51, "excl_end": 56}, {"label": "rev_idxs_37", "start": 56, "excl_end": 59}, {"label": "rev_idxs_39", "start": 59, "excl_end": 62}, {"label": "rev_idxs_40", "start": 62, "excl_end": 65}, {"label": "rev_idxs_41", "start": 65, "excl_end": 68}, {"label": "rev_idxs_42", "start": 68, "excl_end": 75}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 3}, {"label": "tt_segment_1", "start": 3, "excl_end": 13}, {"label": "tt_segment_2", "start": 13, "excl_end": 19}, {"label": "tt_segment_3", "start": 19, "excl_end": 28}, {"label": "tt_segment_4", "start": 28, "excl_end": 43}, {"label": "tt_segment_5", "start": 43, "excl_end": 50}, {"label": "tt_segment_6", "start": 50, "excl_end": 52}, {"label": "tt_segment_7", "start": 52, "excl_end": 59}, {"label": "tt_segment_8", "start": 59, "excl_end": 63}, {"label": "tt_segment_9", "start": 63, "excl_end": 65}, {"label": "tt_segment_10", "start": 65, "excl_end": 75}], "entity_grid": null}}, {"review_id": "H1gOvMYT37", "review_sentences": ["* Summary\n", "This paper addresses machine reading tasks involving tracking the states of entities over text.", "To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module.", "The paper reports positive evaluations on three different tasks.\n\n", "* Review\n\n", "This is an interesting paper.", "The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far.\n\n", "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "This is especially the case in a few places involving coreference:\n", "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.\n", "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.\n\n", "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.\n\n", "Why does the graph update require coreference pooling again?", "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?\n\n", "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?\n\n", "That the model implicitly learns constraints from data is interesting!\n\n", "Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated."], "rebuttal_sentences": ["Thanks for the insightful comments.", "We\u2019ve tried to improve our paper based on your feedback.", "Most significantly, we\u2019ve performed additional ablation studies to confirm that our modeling choices improve performance, and we provide further empirical insight on what the coreference operations do.", "We\u2019ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "Below we address your concerns point-by-point.\n\n", "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "This is especially the case in a few places involving coreference:\n", "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.\n", "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.\n", "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.\n", "======\n", "Based on your comments, we\u2019ve performed additional ablations to measure the impact of the co-reference mechanisms.", "We find that removing any of them leads to a decrease in performance (Rows 2, 3, 4 of Table 5).\n\n", "To provide more than just this quantitative insight, we\u2019ll expand here on how KG-MRC handles coreference to better motivate the modeling choices:\n", "The construction of graph G_t from G_{t-1} uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies.", "We perform coreference disambiguation between location nodes of G_t and G_{t-1} via Eq. 1 (call this inter-graph coreference) and between the location nodes in the same graph Gt (call this intra-graph coreference) via Eq. 2.", "The inter-graph coreference yields new, intermediate representations for the nodes in G_t.", "These are further updated via the intra-graph coreference step.\n\n", "Inter-graph Co-ref: One way to think about this is that we construct a new graph G_t at every time step.", "Now the graph G_{t-1} might contain some location nodes which are predicted again at time step \u2018t\u2019 (e.g., in Figure 2, leaf node already existed in G_{t-1}).", "Instead of replacing an old node with an entirely new node at \u2018t\u2019, we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018t\u2019.\n\n", "Intra-graph Co-ref: Inter-graph co-ref isn\u2019t enough since the MRC module makes its span predictions independently.", "This means that, at time step t, the model could predict the same span/location for multiple entities and add all these duplicates to the graph.", "Moreover, a single location might have the same surface form but be from different parts of the paragraph (e.g. \u201cleaf\u201d in the 1st and the 5th sentence of the para in figure 2).", "The operations in Eq. 2 resolve this by performing self-attention (i.e., the predicted locations of all entities are compared to each other).\n", "=====\n\n", "Response continued from above.\n\n", "Why does the graph update require coreference pooling again?", "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?\n", "=====\n", "We agree that the coreference pooling in the graph update seems repetitive at first glance.", "We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.", "This step does indeed repeat Eq. 2.", "In a nutshell, this is necessary because, after the recurrent and residual graph updates (Eqs 3.1 - 3.3) that propagate information across edges, we may end up with different representations for location nodes corresponding to the same location.", "We don\u2019t want these representations to diverge from each other because of information propagation.\n\n", "To give you more detail:\n", "The graph update step ensures information propagation between entities and location representations.", "Specifically if the current location of entity \u201ce_t\u201d is predicted as \u201c\\lambda_t\u201d, the graph update steps ensures that both the entity and location representation gets the same update (via eq 3.2 and 3.3).", "This would have been sufficient if every entity had a unique location.", "But, multiple entities can actually exist in the same location.", "Let\u2019s consider this small graph below\n\n", "Water - -> leaf\n", "CO_2 --> leaf\n\n", "Here both water and CO_2 exist in the same location, leaf.", "But let\u2019s say that the MRC model picked the \u201cleaf\u201d span from sentence 1 (of the text in Fig 2) for \u201cWater\u201d and from sentence 4 for CO_2.", "In reality, they refer to the same location entity \u201cleaf\u201d.", "Now, due to eq. 3.3, the two embeddings of leaf will get two different residual updates (one would be corresponding to Water and other would be because of CO_2).", "Because of the different updates, the two representations of the same entity might diverge.", "To remedy this, we re-use the coreference matrix \u201cU\u201d we create in eq. (2), which should already have a high attention score corresponding to the two leaf locations.", "Thus we perform a similar operation to the intra-graph update.\n", "====\n", "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?\n", "====\n", "The \u201cprefixes\u201d that our model reads at each time step comprise all sentences up to and including the current sentence s_t.", "The motivation for this modeling choice was empirical.", "In our preliminary experiments we evaluated alternative strategies, such as (a) only considering the current sentence s_t, and (b) considering the entire paragraph at every time step.", "We found that operating on prefixes performed best.", "This is in line with the findings of Dalvi et al., 2018, where the Pro-Global model (which uses prefixes) performs better than the Pro-Local model (which operates on single sentences)."], "review_segmentations": {"label": [{"label": "arg_structuring|heading", "start": 0, "excl_end": 1}, {"label": "arg_structuring|summary", "start": 1, "excl_end": 4}, {"label": "arg_structuring|heading", "start": 4, "excl_end": 5}, {"label": "arg_evaluative", "start": 5, "excl_end": 6}, {"label": "arg_structuring|summary", "start": 6, "excl_end": 7}, {"label": "arg_evaluative", "start": 7, "excl_end": 8}, {"label": "arg_fact", "start": 8, "excl_end": 9}, {"label": "arg_structuring|quote", "start": 9, "excl_end": 11}, {"label": "arg_request|result", "start": 11, "excl_end": 12}, {"label": "arg_request|explanation", "start": 12, "excl_end": 15}, {"label": "arg_evaluative", "start": 15, "excl_end": 16}, {"label": "arg_structuring|summary", "start": 16, "excl_end": 17}], "alignment": [{"label": "reb_idxs_6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25", "start": 7, "excl_end": 12}, {"label": "reb_idxs_28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50", "start": 12, "excl_end": 14}, {"label": "reb_idxs_52|53|54|55|56|57|58", "start": 14, "excl_end": 15}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 7}, {"label": "tt_segment_1", "start": 7, "excl_end": 12}, {"label": "tt_segment_2", "start": 12, "excl_end": 17}], "entity_grid": [{"label": "paper", "start": 1, "excl_end": 16}, {"label": "tasks", "start": 1, "excl_end": 3}, {"label": "states", "start": 1, "excl_end": 6}, {"label": "entities", "start": 1, "excl_end": 10}, {"label": "text", "start": 1, "excl_end": 14}, {"label": "it", "start": 2, "excl_end": 11}, {"label": "knowledge", "start": 2, "excl_end": 6}, {"label": "graph", "start": 2, "excl_end": 12}, {"label": "updates", "start": 2, "excl_end": 13}, {"label": "representation", "start": 2, "excl_end": 9}, {"label": "coreference", "start": 6, "excl_end": 12}, {"label": "modeling", "start": 7, "excl_end": 16}, {"label": "choices", "start": 7, "excl_end": 16}, {"label": "eq", "start": 9, "excl_end": 13}, {"label": "model", "start": 14, "excl_end": 15}]}, "rebuttal_segmentations": {"label": [{"label": "rebuttal_social", "start": 0, "excl_end": 2}, {"label": "rebuttal_summary", "start": 2, "excl_end": 5}, {"label": "rebuttal_structuring", "start": 5, "excl_end": 12}, {"label": "rebuttal_done", "start": 12, "excl_end": 13}, {"label": "rebuttal_answer", "start": 13, "excl_end": 26}, {"label": "rebuttal_structuring", "start": 26, "excl_end": 31}, {"label": "rebuttal_concede-criticism", "start": 31, "excl_end": 32}, {"label": "rebuttal_done", "start": 32, "excl_end": 33}, {"label": "rebuttal_structuring", "start": 33, "excl_end": 34}, {"label": "rebuttal_answer", "start": 34, "excl_end": 36}, {"label": "rebuttal_structuring", "start": 36, "excl_end": 37}, {"label": "rebuttal_answer", "start": 37, "excl_end": 41}, {"label": "rebuttal_structuring", "start": 41, "excl_end": 42}, {"label": "rebuttal_answer", "start": 42, "excl_end": 51}, {"label": "rebuttal_structuring", "start": 51, "excl_end": 54}, {"label": "rebuttal_answer", "start": 54, "excl_end": 59}], "alignment": [{"label": "rev_idxs_7|8|9|10|11", "start": 6, "excl_end": 26}, {"label": "rev_idxs_12|13", "start": 28, "excl_end": 51}, {"label": "rev_idxs_14", "start": 52, "excl_end": 59}], "text_tiling": [{"label": "tt_segment_0", "start": 0, "excl_end": 6}, {"label": "tt_segment_1", "start": 6, "excl_end": 14}, {"label": "tt_segment_2", "start": 14, "excl_end": 19}, {"label": "tt_segment_3", "start": 19, "excl_end": 22}, {"label": "tt_segment_4", "start": 22, "excl_end": 27}, {"label": "tt_segment_5", "start": 27, "excl_end": 42}, {"label": "tt_segment_6", "start": 42, "excl_end": 59}], "entity_grid": [{"label": "comments", "start": 0, "excl_end": 12}, {"label": "we", "start": 1, "excl_end": 57}, {"label": "paper", "start": 1, "excl_end": 8}, {"label": "ablation", "start": 2, "excl_end": 32}, {"label": "modeling", "start": 2, "excl_end": 55}, {"label": "choices", "start": 2, "excl_end": 14}, {"label": "performance", "start": 2, "excl_end": 13}, {"label": "insight", "start": 2, "excl_end": 14}, {"label": "coreference", "start": 2, "excl_end": 49}, {"label": "operations", "start": 2, "excl_end": 25}, {"label": "model", "start": 3, "excl_end": 58}, {"label": "section", "start": 3, "excl_end": 4}, {"label": "mechanisms", "start": 3, "excl_end": 12}, {"label": "table", "start": 4, "excl_end": 32}, {"label": "it", "start": 6, "excl_end": 10}, {"label": "eq", "start": 8, "excl_end": 49}, {"label": "node", "start": 8, "excl_end": 21}, {"label": "representation", "start": 8, "excl_end": 38}, {"label": "self", "start": 9, "excl_end": 25}, {"label": "attention", "start": 9, "excl_end": 49}, {"label": "location", "start": 9, "excl_end": 46}, {"label": "entities", "start": 9, "excl_end": 40}, {"label": "co-reference", "start": 12, "excl_end": 15}, {"label": "mrc", "start": 14, "excl_end": 45}, {"label": "graph", "start": 15, "excl_end": 41}, {"label": "g_t", "start": 15, "excl_end": 19}, {"label": "g", "start": 15, "excl_end": 20}, {"label": "t", "start": 15, "excl_end": 23}, {"label": "disambiguation", "start": 15, "excl_end": 16}, {"label": "nodes", "start": 15, "excl_end": 34}, {"label": "inter-graph", "start": 16, "excl_end": 17}, {"label": "representations", "start": 17, "excl_end": 48}, {"label": "step", "start": 18, "excl_end": 56}, {"label": "co-ref", "start": 19, "excl_end": 22}, {"label": "time", "start": 19, "excl_end": 56}, {"label": "figure", "start": 20, "excl_end": 24}, {"label": "leaf", "start": 20, "excl_end": 49}, {"label": "update", "start": 21, "excl_end": 50}, {"label": "information", "start": 21, "excl_end": 37}, {"label": "steps", "start": 21, "excl_end": 38}, {"label": "span", "start": 22, "excl_end": 45}, {"label": "paragraph", "start": 24, "excl_end": 56}, {"label": "sentence", "start": 24, "excl_end": 56}, {"label": "locations", "start": 25, "excl_end": 49}, {"label": "updates", "start": 29, "excl_end": 48}, {"label": "text", "start": 32, "excl_end": 52}, {"label": "propagation", "start": 35, "excl_end": 37}, {"label": "entity", "start": 38, "excl_end": 48}, {"label": "water", "start": 42, "excl_end": 47}, {"label": "co_2", "start": 43, "excl_end": 47}, {"label": "choice", "start": 52, "excl_end": 55}, {"label": "prefixes", "start": 52, "excl_end": 58}, {"label": "sentences", "start": 54, "excl_end": 58}, {"label": "s_t", "start": 54, "excl_end": 56}]}}]